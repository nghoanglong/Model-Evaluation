{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis - LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMks64oh8efF0g8TyBBPJ2E",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nghoanglong/NLP-Sentiment-Analysis/blob/master/Sentiment_Analysis_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmb7FNykAHVJ"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import csv\n",
        "import chardet\n",
        "nltk.download('punkt')\n",
        "from nltk.tree import Tree\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsESg0MMBL9Z"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDN8KpuAChef"
      },
      "source": [
        "class PreprocessData:\n",
        "    def load_dataset(self,\n",
        "                     DATASET_REQ_PATH):\n",
        "        \"\"\"Load dataset tương ứng theo type_dataset\n",
        "\n",
        "            type_dataset = [train, dev, test] | type = string\n",
        "            file_extension = .txt, .csv,... | type = string\n",
        "\n",
        "            return ndarray shape(n, 2) với row = array[sentence, label]\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(DATASET_REQ_PATH, 'rb') as f:\n",
        "                result = chardet.detect(f.read())\n",
        "            dataset = pd.read_csv(DATASET_REQ_PATH, encoding=result['encoding'])\n",
        "            dataset = dataset.to_numpy()\n",
        "            return dataset\n",
        "        except FileExistsError as err:\n",
        "            print(err)\n",
        "            return None\n",
        "\n",
        "    def transform_sentence(self,\n",
        "                           sent_tokenized,\n",
        "                           li_vocabs,\n",
        "                           mode=True):\n",
        "        \"\"\"Encode và decode một sentence về dạng tương ứng\n",
        "\n",
        "           if mode = True => Encode một sentence đã được tokenize về dạng numerical\n",
        "              sent_tokenized: [token, token, token,...]\n",
        "              li_vocabs = {token: id, token: id,...}\n",
        "              return sentence = [id, id, id, id,...]\n",
        "\n",
        "           if mode = False => Decode một sentence ở dạng numerical về dạng list các tokens\n",
        "              sent_tokenized: [id, id, id,...]\n",
        "              li_vocabs = {id: token, id: token, id: token,...}\n",
        "              return sentence = [token, token, token, token,...]\n",
        "        \"\"\"\n",
        "        oov_tok = \"<OOV>\"\n",
        "        sent_transformed = None\n",
        "        if mode:\n",
        "            # nếu token ko có trong li_vocabs -> thêm idx của '<OOV>'\n",
        "            sent_transformed = np.array([li_vocabs.get(token, li_vocabs[oov_tok])\n",
        "                                         for token in sent_tokenized])\n",
        "        else:\n",
        "            # nếu idx ko có trong li_vocabs -> là giá trị pad_value đc thêm vào -> remove all pad_value\n",
        "            transformed = np.array([li_vocabs.get(idx, 'pad_value')\n",
        "                                    for idx in sent_tokenized])\n",
        "            sent_transformed = np.delete(transformed, \n",
        "                                         np.where(transformed == 'pad_value'))\n",
        "        return sent_transformed\n",
        "    def visualize_sentence_length(self,\n",
        "                                  dataset):\n",
        "      \"\"\"Visualize length of all sequences in the dataset\n",
        "      \"\"\"\n",
        "      df = pd.DataFrame(np.array([len(sample) for sample in dataset]), \n",
        "                        columns=['length'])\n",
        "      _, axes = plt.subplots(figsize=(25, 5))\n",
        "      sns.countplot(x='length', data=df, ax=axes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uNLezMaKcwq"
      },
      "source": [
        "# load dataset\n",
        "data = PreprocessData()\n",
        "loaded_train_data = data.load_dataset('/content/gdrive/MyDrive/All Datasets/NLP-Sentiment-data/data pre-processed/train_csv.csv')\n",
        "loaded_dev_data = data.load_dataset('/content/gdrive/MyDrive/All Datasets/NLP-Sentiment-data/data pre-processed/dev_csv.csv')\n",
        "loaded_test_data = data.load_dataset('/content/gdrive/MyDrive/All Datasets/NLP-Sentiment-data/data pre-processed/test_csv.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFQQjDSwFDk3"
      },
      "source": [
        "train_data = loaded_train_data[:,0]\n",
        "dev_data = loaded_dev_data[:, 0]\n",
        "test_data = loaded_test_data[:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kIqGu5NcaPoj"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "train_data_label = np.array(loaded_train_data[:, 1]).astype(np.int64)\n",
        "dev_data_label = np.array(loaded_dev_data[:, 1]).astype(np.int64)\n",
        "test_data_label = np.array(loaded_test_data[:, 1]).astype(np.int64)\n",
        "\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "lb.fit(np.concatenate((train_data_label, dev_data_label, test_data_label),axis=0))\n",
        "\n",
        "train_data_label = lb.transform(train_data_label)\n",
        "dev_data_label = lb.transform(dev_data_label)\n",
        "test_data_label = lb.transform(test_data_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLlJ4xnker7v"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwuR3Tpb_n-9"
      },
      "source": [
        "oov_tok = '<OOV>'\n",
        "MAX_TOKENS = 10000\n",
        "# build vocabulary trên các sentence từ 3 bộ data\n",
        "tokenizer = Tokenizer(num_words=MAX_TOKENS, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(np.concatenate((train_data, dev_data, test_data), axis=0))\n",
        "li_vocabs = tokenizer.word_index\n",
        "\n",
        "# encode các sentence về dạng ids\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
        "dev_sequences = tokenizer.texts_to_sequences(dev_data)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i3Xm_yuCJsn"
      },
      "source": [
        "data.visualize_sentence_length(train_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5dqf7nZDPJ4"
      },
      "source": [
        "data.visualize_sentence_length(dev_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpCiy37gDTxI"
      },
      "source": [
        "data.visualize_sentence_length(test_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMoNpaLPeAgV"
      },
      "source": [
        "# Sau khi\n",
        "max_length_seq = 53\n",
        "padding_type = 'post'\n",
        "trungcating_type = 'post'\n",
        "\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length_seq, padding=padding_type, truncating=trungcating_type)\n",
        "dev_padded = pad_sequences(dev_sequences, maxlen=max_length_seq, padding=padding_type, truncating=trungcating_type)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length_seq, padding=padding_type, truncating=trungcating_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsyakoHKG2iQ"
      },
      "source": [
        "# build model\n",
        "EMBEDDING_DIM = 64\n",
        "NUM_WORDS = len(li_vocabs)\n",
        "model = keras.Sequential([\n",
        "            tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_DIM, input_length=max_length_seq),\n",
        "            tf.keras.layers.SpatialDropout1D(0.2),\n",
        "            tf.keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
        "            tf.keras.layers.Dense(5, activation='softmax')        \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVvJB79csYxU"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RdiMhaCshU5"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJNc4D6xszyt"
      },
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(train_padded, train_data_label, epochs=num_epochs, validation_data=(test_padded, test_data_label), verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4quOQ3etYoM"
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AASDpMt4tcaO"
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}