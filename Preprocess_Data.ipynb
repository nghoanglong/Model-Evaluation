{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocess Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nghoanglong/NLP-Sentiment-Analysis/blob/master/Preprocess_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgUufpHBkDI2"
      },
      "source": [
        "## Requirements\n",
        "reader: PATH_FOLDER, split_type=['train', 'test', 'dev']\n",
        "    \n",
        "   + return: matrix\n",
        "    \n",
        "   + với matrix: row - samples dạng sentence\n",
        "    \n",
        "embedding: sentences to id\n",
        "\n",
        "   + sample sentence return array of indices [2, 4, 6, 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoyvvBpwkJlK"
      },
      "source": [
        "## Knowledges\n",
        "+ Word Embedding \n",
        "    + frequency-based\n",
        "        + one-hot vector\n",
        "        + tf-idf\n",
        "        + co-occurence matrix\n",
        "    + prediction-based\n",
        "        + CBOW\n",
        "        + Skip-gram\n",
        "+ Dense vector embedding -> neural net\n",
        "+ NLTK xử lý sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZD_3Ue6kLEA"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tree import Tree\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXU6HPC7gdGm"
      },
      "source": [
        "from google.colab import files \n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L787lZzjkOuu"
      },
      "source": [
        "class PreprocessData:\n",
        "    def __init__(self,\n",
        "                 FOLDER_PATH):\n",
        "        \"\"\"Constructor với tham số nhận vào làm FOLDER_PATH\n",
        "\n",
        "            FOLDER_PATH định dạng ví dụ: dir_current/data_set/\n",
        "        \"\"\"\n",
        "        self.folder_path = FOLDER_PATH\n",
        "        self.dataset = None\n",
        "        self.lib_tokens = None\n",
        "\n",
        "    def load_dataset(self,\n",
        "                     type_dataset,\n",
        "                     file_extension='.txt'):\n",
        "        \"\"\"Load dataset tương ứng theo type_dataset\n",
        "\n",
        "            type_dataset = [train, dev, test] | type = string\n",
        "            file_extension = .txt, .csv,... | type = string\n",
        "\n",
        "            return ndarray shape(n, 2) với\n",
        "                row = np.array[list_tokens extract from sentence, label]\n",
        "        \"\"\"\n",
        "        # try:\n",
        "        #     DATASET_REQ_PATH = self.folder_path + '/' + type_dataset + file_extension\n",
        "        #     check_exist = os.path.isfile(DATASET_REQ_PATH)\n",
        "        #     if check_exist:\n",
        "        DATASET_REQ_PATH = 'train.txt'\n",
        "        with open(DATASET_REQ_PATH, 'r') as reader:\n",
        "            self.dataset = np.array([\n",
        "                                    np.array([self.PTB_tokenize(line.rstrip(\"\\n\")),\n",
        "                                              self.PTB_get_label(line)], dtype=object)\n",
        "                                    for line in reader])\n",
        "        return self.dataset\n",
        "        #     else:\n",
        "        #         raise FileExistsError('File nay ko ton tai')\n",
        "        # except FileExistsError as err:\n",
        "        #     print(err)\n",
        "        #     return None\n",
        "\n",
        "    def PTB_get_label(self,\n",
        "                      treebank):\n",
        "        \"\"\"get label của root sentece trong PTB\n",
        "            treebank - type string\n",
        "            return label\n",
        "        \"\"\"\n",
        "        tree = Tree.fromstring(treebank)\n",
        "        return tree.label()\n",
        "\n",
        "    def PTB_tokenize(self,\n",
        "                     treebank):\n",
        "        \"\"\"Split list các token từ cây PTB\n",
        "            \n",
        "            treebank - type string\n",
        "            return array = [token, token, token,...]\n",
        "        \"\"\"\n",
        "        tree = Tree.fromstring(str(treebank))\n",
        "        return np.array(tree.leaves())\n",
        "\n",
        "    def transfrom_sentence(self,\n",
        "                           li_tokens):\n",
        "        \"\"\"Transfrom list các tokens thành 1 sentence hoàn chỉnh\n",
        "\n",
        "            li_tokens = [token, token, token,...]\n",
        "            return sentence\n",
        "        \"\"\"\n",
        "        sentence = ' '.join(li_tokens)\n",
        "        return sentence\n",
        "\n",
        "    def get_list_vocabularies(self):\n",
        "        \"\"\"Tạo một kho các tokens từ list các sentences\n",
        "\n",
        "            return dictionary{token: ids, token: ids,...}\n",
        "        \"\"\"\n",
        "        li_tokens = set()\n",
        "        for sample in self.dataset:\n",
        "            li_tokens.update(sample[0])\n",
        "        lib_tokens = dict([(token, idx)\n",
        "                           for idx, token in enumerate(li_tokens)])\n",
        "        return lib_tokens\n",
        "\n",
        "    def encode_sentence(self,\n",
        "                        sent_tokenized,\n",
        "                        li_vocabs):\n",
        "        \"\"\"Encode một sentence về dạng mỗi token tương ứng với một id trong list vocabs\n",
        "\n",
        "            sent_tokenized - sentence đã được tokenize thành list các tokens\n",
        "            li_vocabs = {token: id, token: id,...}\n",
        "            return sentence = [id, id, id, id,...]\n",
        "        \"\"\"\n",
        "\n",
        "        res_encode = np.array([li_vocabs[token] if token in li_vocabs\n",
        "                               else print('does not have token in li vocabs')\n",
        "                               for token in sent_tokenized])\n",
        "        return res_encode\n",
        "\n",
        "    def decode_sentence(self,\n",
        "                        li_vocabs):\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB8LouuDkUGz"
      },
      "source": [
        "data = PreprocessData('./data/trees')\n",
        "train_data = data.load_dataset('train', '.txt')\n",
        "\n",
        "li_vocabs = data.get_list_vocabularies() # lấy ra list các vocabs\n",
        "encode_li_sent = np.array([torch.tensor(data.encode_sentence(sample, li_vocabs))\n",
        "                            for sample in train_data[:, 0]],\n",
        "                          dtype=object) # encode list các sentence\n",
        "print(encode_li_sent.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}