{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocess Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZHxoovqIO/sXjEr9BgzH2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nghoanglong/NLP-Sentiment-Analysis/blob/master/Preprocess_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13aUkezvj9yQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgUufpHBkDI2"
      },
      "source": [
        "## Requirements\n",
        "reader: PATH_FOLDER, split_type=['train', 'test', 'dev']\n",
        "    \n",
        "   + return: matrix\n",
        "    \n",
        "   + với matrix: row - samples dạng sentence\n",
        "    \n",
        "embedding: sentences to id\n",
        "\n",
        "   + sample sentence return array of indices [2, 4, 6, 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoyvvBpwkJlK"
      },
      "source": [
        "## Knowledges\n",
        "+ Word Embedding \n",
        "    + frequency-based\n",
        "        + one-hot vector\n",
        "        + tf-idf\n",
        "        + co-occurence matrix\n",
        "    + prediction-based\n",
        "        + CBOW\n",
        "        + Skip-gram\n",
        "+ Dense vector embedding -> neural net\n",
        "+ NLTK xử lý sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZD_3Ue6kLEA"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from nltk.tree import Tree\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L787lZzjkOuu"
      },
      "source": [
        "class PreprocessData:\n",
        "    def __init__(self,\n",
        "                 FOLDER_PATH):\n",
        "        \"\"\"Constructor với tham số nhận vào làm FOLDER_PATH\n",
        "\n",
        "            FOLDER_PATH định dạng ví dụ: dir_current/data_set/\n",
        "        \"\"\"\n",
        "        self.folder_path = FOLDER_PATH\n",
        "        self.dataset = None\n",
        "        self.lib_tokens = None\n",
        "\n",
        "    def load_dataset(self,\n",
        "                     type_dataset,\n",
        "                     file_extension='.txt'):\n",
        "        \"\"\"Load dataset tương ứng theo type_dataset\n",
        "\n",
        "            type_dataset = [train, dev, test] | type = string\n",
        "            file_extension = .txt, .csv,... | type = string\n",
        "\n",
        "            return ndarray shape(n, 1) với\n",
        "                row = sentence được format theo treebank\n",
        "        \"\"\"\n",
        "        try:\n",
        "            DATASET_REQ_PATH = self.folder_path + '/' + type_dataset + file_extension\n",
        "            check_exist = os.path.isfile(DATASET_REQ_PATH)\n",
        "            if check_exist:\n",
        "                with open(DATASET_REQ_PATH, 'r') as reader:\n",
        "                    self.dataset = np.array(\n",
        "                        [line.rstrip(\"\\n\") for line in reader])\n",
        "                return self.dataset\n",
        "            else:\n",
        "                raise FileExistsError('File nay ko ton tai')\n",
        "        except FileExistsError as err:\n",
        "            print(err)\n",
        "            return None\n",
        "\n",
        "    def SplitToken_FromTreebank(self,\n",
        "                                treebank):\n",
        "        \"\"\"Split list các token từ TreeBank\n",
        "            \n",
        "            treebank - type string\n",
        "            return array = [token, token, token,...]\n",
        "        \"\"\"\n",
        "        tree = Tree.fromstring(str(treebank))\n",
        "        return tree.leaves()\n",
        "\n",
        "    def Tree_toSentence(self,\n",
        "                        treebank):\n",
        "        \"\"\"Convert tree thành một sentence hoàn chỉnh với\n",
        "\n",
        "            treebank - type string\n",
        "            return sentence\n",
        "        \"\"\"\n",
        "        sentence = ' '.join(self.SplitToken_FromTreebank(treebank))\n",
        "        return sentence\n",
        "\n",
        "    def getAllTokens(self):\n",
        "        \"\"\"Tạo một kho các tokens từ list các sentences\n",
        "\n",
        "            return dictionary{token: ids, token: ids,...}\n",
        "        \"\"\"\n",
        "        big_sent = ' '.join(self.Tree_toSentence(sentence) for sentence in self.dataset)\n",
        "        li_tokens = word_tokenize(big_sent)\n",
        "        self.lib_tokens = {}\n",
        "        ids = 1\n",
        "        for token in li_tokens:\n",
        "            if token not in self.lib_tokens:\n",
        "                self.lib_tokens[token] = ids\n",
        "                ids = ids + 1\n",
        "        return self.lib_tokens\n",
        "        \n",
        "\n",
        "    def assign_sentiment(self,\n",
        "                         file_phrases,\n",
        "                         file_sentimentLabels):\n",
        "        \"\"\"Gán sentiment labels cho các phrases tương ứng\n",
        "\n",
        "            Kết quả là 1 DataFrame được lưu thành file csv với định dạng\n",
        "                row = samples\n",
        "                column = ['phrase ids', 'phrases', 'sentiment values']\n",
        "\n",
        "        \"\"\"\n",
        "        df_phrases = pd.read_csv(file_phrases, sep='|', header=None)\n",
        "        df_sentimentLabels = pd.read_csv(file_sentimentLabels, sep='|')\n",
        "\n",
        "        df_phrases.columns = ['phrases', 'phrase ids']\n",
        "\n",
        "        df_assignLabels = pd.merge(\n",
        "            df_phrases, df_sentimentLabels, on='phrase ids')\n",
        "        df_assignLabels = df_assignLabels[[\n",
        "            'phrase ids', 'phrases', 'sentiment values']]\n",
        "\n",
        "        # create and save data to folder\n",
        "        try:\n",
        "            path = os.getcwd() + \"\\data\"\n",
        "            if os.path.exists(path):\n",
        "                raise OSError\n",
        "            else:\n",
        "                os.mkdir(path)\n",
        "        except OSError:\n",
        "            print(f\"Can't create folder at {path} because it was existed\")\n",
        "        else:\n",
        "            print(f\"Successfully created folder at {path}\")\n",
        "        finally:\n",
        "            print(f'Create and save file data at {path}')\n",
        "            df_assignLabels.to_csv(path + \"\\\\phrases_and_sentiment.csv\")\n",
        "\n",
        "    def split_dataset(self,\n",
        "                      SENTENCE_PATH,\n",
        "                      LABEL_SENTENCE_PATH):\n",
        "        \"\"\"Chia dataset gồm các sentence thành các file data khác nhau dựa theo label, rồi tạo folder để lưu trữ file\n",
        "        \n",
        "            label 1 = train data\n",
        "            label 2 = test data\n",
        "            label 3 = dev data\n",
        "        \"\"\"\n",
        "\n",
        "        df_dataSentence = pd.read_csv(SENTENCE_PATH, sep='\\t')\n",
        "        df_labelSplitData = pd.read_csv(LABEL_SENTENCE_PATH, sep=',')\n",
        "        df_mergeSplitLabel = pd.merge(df_dataSentence,\n",
        "                                      df_labelSplitData,\n",
        "                                      on='sentence_index')\n",
        "\n",
        "        grp_splitlabel = df_mergeSplitLabel.groupby('splitset_label')\n",
        "        train_data = grp_splitlabel.get_group(1).loc[:, ['sentence', 'splitset_label']]\n",
        "        test_data = grp_splitlabel.get_group(2).loc[:, ['sentence', 'splitset_label']]\n",
        "        dev_data = grp_splitlabel.get_group(3).loc[:, ['sentence', 'splitset_label']]\n",
        "\n",
        "        datasets = np.array(\n",
        "            [train_data, test_data, dev_data],\n",
        "            dtype=object\n",
        "        )\n",
        "\n",
        "        # thêm cột index [0, 1, 2, 3,...]\n",
        "        for file in datasets:\n",
        "            file.index = pd.MultiIndex.from_arrays(\n",
        "                [np.arange(len(file.index))], names=['index'])\n",
        "\n",
        "        # create and save data to folder\n",
        "        try:\n",
        "            path = os.getcwd() + \"\\data\"\n",
        "            if os.path.exists(path):\n",
        "                raise OSError\n",
        "            else:\n",
        "                os.mkdir(path)\n",
        "        except OSError:\n",
        "            print(f\"Can't create folder at {path} because it was existed\")\n",
        "        else:\n",
        "            print(f\"Successfully created folder at {path}\")\n",
        "        finally:\n",
        "            print(f'Create and Save file data at {path}')\n",
        "            train_data.to_csv(path + \"\\\\train_data.csv\")\n",
        "            dev_data.to_csv(path + \"\\\\dev_data.csv\")\n",
        "            test_data.to_csv(path + \"\\\\test_data.csv\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB8LouuDkUGz"
      },
      "source": [
        "data = PreprocessData('./data/trees')\n",
        "train_data = data.load_dataset('train', '.txt')\n",
        "\n",
        "lib_tokens = data.getAllTokens()\n",
        "print(len(lib_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}